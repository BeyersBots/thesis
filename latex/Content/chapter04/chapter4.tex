\documentclass[12pt,letterpaper,oneside]{book}
\usepackage{../../afitStyleFiles/afitThesis}
\usepackage[nolist]{acronym}
\usepackage{todo}
\usepackage{tabu}
\usepackage{makecell}
\renewcommand\theadfont{\bfseries}
\graphicspath{{../../Figures/}}
\input{../Preamble/myFigures}
\input{../Preamble/myTables}
\usepackage[outdir=./]{epstopdf}

\begin{document}
	\begin{acronym}
		\acro {BLE} {Bluetooth Low Energy}
		\acro {SIG} {Special Interest Group}
		\acro {BR/EDR} {Basic Rate/Enhanced Data Rate}
		\acro {IoT} {internet of things}
		\acro {CI} {critical infrastructure}
		\acro {WSN} {Wireless Sensor Network}
		\acro {ATT} {Attribute Protocol}
		\acro {GATT} {Generic Attribute Profile}
		\acro {SM} {Security Manager}
		\acro {LTK} {Long Term Key}
		\acro {CSRK} {Connection Signature Resolving Key}
		\acro {IRK} {Identity Resolving Key}
		\acro {AES} {Advanced Encryption Standard}
		\acro {TK} {Temporary Key}
		\acro {STK} {Short-Term Key}
		\acro {ECDH} {Elliptic Curve Diffie Hellman}
		\acro {CE} {Connection Events}
		\acro {CRC} {Cyclic Redundancy Check}
		\acro {SCA} {sleep clock accuracy}
		\acro {RSSI} {Received Signal Strength Indicator}
		\acro {MAC} {Media Access Control}
		\acro {CRM} {Customer Relationship Management}
		\acro {SSID} {Service Set Identifiers}
		\acro {AP} {access point}
		\acro {CITIoT} {Classification, Identification, and Tracking of \ac{IoT}}
		\acro {MIoTL} {Mitigation of \ac{IoT} Leakage}
		\acro {SHAA} {Smart Home Automation Architecture}
		\acro {SUT} {System Under Test}
		\acro {CUT} {Component Under Test}
		\acro {DCSR} {Device Classification Success Rate}
		\acro{EISR} {Event Identification Success Rate}
		\acro{EIFP} {Event Identification False Positives}
		\acro{EIFN} {Event Identification False Negatives}
		\acro{ULSR} {User Location Success Rate}
		\acro{PT} {Processing Time}
		\acro{HDU} {Hard Drive Usage}
		\acro{COTS} {commercial off-the-shelf}
	\end{acronym}
\mainmatter
	\chapter{Methodology}
		\section{Problem/Objective}
		This research aims to demonstrate how data leakage in smart home environments enable an eavesdropper to classify \ac{IoT} devices, track user's movements, map networks, and identify events within the smart home. It also seeks to show how a smart home user can defend against these attacks. These goals are enabled through the implementation of the \ac{CITIoT} and \ac{MIoTL} tools respectively. The experiment presented in this section functions as an evaluation of these tools in a realistic smart home environment, testing how accurately the \ac{CITIoT} tool operates against the \ac{SHAA} and how well \ac{MIoTL} mitigates these attacks. Specifically, the experimentation attempts to accomplish four objectives:
		\begin{enumerate}
			\item Determine the ability of an observer to accurately classify smart home devices.*
			\item Examine with what success rate events can be identified.*
			\item Measure the capability to track when users are in the smart home.*
			\item Evaluate processing time and storage requirements.
			
			\begin{footnotesize}
			* Objective is fulfilled in two states: when the \ac{MIoTL} tool is activated and when it is not.
			\end{footnotesize}
		\end{enumerate}
		The evaluation results will provide consumers with an understanding of data leakage in smart home environments and a method to defend against these vulnerabilities.
		
		\section{System Under Test}
		Figure~\ref{fig:SutCutDiagram} displays the \ac{SUT} and \ac{CUT} diagram. Section \ref{treatments} describes the experiment treatments which include a user performing actions from a script to interact with the smart home environment and the operation of the \ac{MIoTL} tool. Section~\ref{constantFactors} discusses the constant factors that do not change throughout the experiment such as computing parameters and the number of devices. The actual Wi-Fi and \ac{BLE} traffic collected by the \ac{CITIoT} tool is considered uncontrolled and is examined in Section~\ref{uncontrolledVariables}. The components tested include the preprocessor, \ac{MAC} tracker, classifier, and network mapper. Response variables, or metrics, described in Section~\ref{responseVariables}, consist of classified devices, identified events, user tracking, processing time, and storage requirements.
		
		\figSutCutDiagram
		
			\subsection{Assumptions}
			The following assumptions are made when designing and executing experiments for the \ac{CITIoT} tool:
			\begin{enumerate}
				\item The actions performed within \ac{SHAA} are representative of a real smart home environment.
				\item The eavesdropper has already accomplished reconnaissance and scanning and has the required parameters to run the \ac{CITIoT} tool.
				\item During experimentation, the \ac{CITIoT} tool is positioned within \ac{SHAA}, whereas in real-world operation it would be outside of the smart home. It is assumed that a directional antenna aimed at the smart home would have similar results to the antennae used within the smart home during this experimentation. This assumption is substantiated through Rose, et al.'s work cracking a gun safe from a quarter mile away \cite{RoseLocks}.
				\item A directional antenna can be used to collect traffic only from the targeted smart home.
				\item The degree of precision for an identified event is one minute. This level of precision provides enough accuracy for the problem presented in this thesis and allows for signal propagation and sniffer delays to be ignored.
			\end{enumerate}
			
		\section{Response Variables} \label{responseVariables}
		
		The objectives of this experiment influence the response variables used in measuring the accuracy and performance of the \ac{CITIoT} tool. While not directly measured, the effectiveness of \ac{MIoTL} is quantified via the observed decrease in the \ac{CITIoT} tool's operation when the mitigation tool is operating. Therefore, response variables (or performance metrics) tied to the four objectives help consider the overall operation of the \ac{CITIoT} tool in both states:
		
		\begin{itemize}
			\item \textbf{Objective 1:} Determine the ability of an observer to accurately classify smart home devices.
				
			\hspace{4ex}\textbf{\ac{DCSR}}: Since the number of devices within \ac{SHAA} is controlled, and the \ac{CITIoT} tool attempts to classify each device, the \ac{DCSR} response variable can measure the tool's ability to accurately classify devices. The \ac{DCSR} metric can be expressed by the simple ratio measurement
			\begin{equation}
			DCSR = \frac{DC}{TD}
			\end{equation}
			where $DC$ represents the number of successfully classified devices and $TD$ represents the total number of devices within \ac{SHAA}.
			
			\item \textbf{Objective 2:} Examine with what success rate events can be identified.
			
			\hspace{4ex}\textbf{\ac{EISR}:} The \ac{EISR} response variable measures the tool's ability to accurately identify events. An event is considered successfully identified if the time and device of the event recognized by the \ac{CITIoT} tool matches an event in the log. The degree of precision for an identified event is one minute. The \ac{EISR} metric can be expressed by the simple ratio measurement
			\begin{equation}
			EISR = \frac{TP}{TE}
			\end{equation}
			where $TP$ represents the number of true positives, or successfully identified events, and $TE$ represents the total number of events per test trial.
			
			\hspace{4ex}\textbf{\ac{EIFP}:} The \ac{EIFP} response variable measures the rate at which the \ac{CITIoT} tool falsely identifies events that did not actually occur. The \ac{EIFP} metric can be expressed by the simple ratio measurement
			\begin{equation}
			EIFP = \frac{FP}{EI}
			\end{equation}
			where $FP$ represents the number of false positives, or identified events that did not occur, and $EI$ represents the total number of events identified by the \ac{CITIoT} tool.
			
			\hspace{4ex}\textbf{\ac{EIFN}:} The \ac{EIFN} response variable measures the rate at which the \ac{CITIoT} tool fails to identify events that did actually occur. The \ac{EIFN} metric can be expressed by the simple ratio measurement
			\begin{equation}
			EIFN = \frac{FN}{TE}
			\end{equation}
			where $FN$ represents the total number of false negatives, or events the tool failed to identify, and $TE$ represents the total number events per test trial. The \ac{EIFN} metric can be simplified to
			\begin{equation}
			EIFN = 1 - EISR
			\end{equation}
			
			\item \textbf{Objective 3:} Measure the capability to track when users are in the smart home.
			
			\hspace{4ex}\textbf{\ac{ULSR}:} The \ac{ULSR} response variable measures the rate at which a user's location is accurately identified as home or away via Wi-Fi devices. The \ac{ULSR} metric can be expressed by the simple ratio measurement
			\begin{equation}
			ULSR = \frac{ST}{TT}
			\end{equation}
			where $ST$ represents the total time (minutes) which the location of the user is successfully tracked and $TT$ is the total time (minutes) of the experiment.
			
			\item \textbf{Objective 4:} Evaluate processing time and storage requirements.
			
			\hspace{4ex}\textbf{\ac{PT}:} The \ac{PT} metric is the average wall-clock processing time across all trials for each separate unit and the \ac{CITIoT} tool as a whole. Each unit and experimental trial analyzes a different number of packets, therefore, the \ac{PT} is normalized by taking the average \ac{PT} of a single trial over 25,000 packets. Next, all of the trial's normalized \ac{PT} values are averaged to provide the average tool and unit \ac{PT} respectively. The average \ac{CITIoT} tool \ac{PT} metric can be expressed by the equation
			\begin{equation}
			CITIoT PT =\frac{\sum_{n=1}^{NumTrials} 25000\times\frac{T_n}{TP_n}} {NumTrials}
			\end{equation}
			where $NumTrials$ represents the number of trials, $T_n$ is the total time of a given trial, and $TP_n$ is the total number of packets in a given trial.
			
			The timing of a given unit is calculated a little differently as multiple units run simultaneously with shared processing time. A unit's \ac{PT}, then, only accounts for time used to assist in that unit's purpose and not other units. For example, the preprocessor and \ac{MAC} tracker both require the packet capture to be read into a list. Therefore, the time taken to parse the capture is accounted for in both unit's \ac{PT}, but the time used to track \ac{MAC} addresses is not added to the preprocessor's \ac{PT}. Only two units operate at a time, so a unit's \ac{PT} can be expressed by the equation
			\begin{equation}
			UnitPT=\frac{\sum_{n=1}^{NumTrials} 25000\times\frac{T_n-T_b}{TP_n}}{NumTrials}
			\end{equation}
			where $T_b$ represents the total processing time exclusive to the other unit's operation.
			
			\hspace{4ex}\textbf{\ac{HDU}:} The \ac{HDU} metric is the amount of hard drive space used by all components of the \ac{CITIoT} tool after operation.
			
		\end{itemize}
	
		Table~\ref{tbl:PerformanceMetrics} defines each performance metric's units of measurement, accepted range value, and expected range value. Objectives one through three and corresponding response variables are also observed while the \ac{MIoTL} tool is operating.
		
		\tablePerformanceMetrics
		
		\section{Control Variables} \label{controlVariables}
		
		A primary goal of this experiment is to observe how the \ac{CITIoT} tool operates in a realistic smart home environment. Using \ac{COTS} components restricts the number of factors that can be altered during experimentation. Event type and timing are the primary factors, and are the main treatments in the experiment. A scripted number of events are performed in a random order and time interval throughout a trial. Additionally, the operating status of the \ac{MIoTL} tool is used to evaluate the \ac{CITIoT} tool's operation during mitigation.
		
		\section{Uncontrolled Variables} \label{uncontrolledVariables}
		
		Another consequence of testing the \ac{CITIoT} tool against a realistic smart home environment is the introduction of uncontrollable factors. The use of \ac{COTS} components and an open environment introduces wireless noise and the occurrence of unscripted events. This is beneficial to the evaluation of the \ac{CITIoT} tool as it is meant to operate among real-world interference. 
		
		\section{Constant Factors} \label{constantFactors}
		
		Throughout the course of experimentation, several factors will be held constant to limit the scope of the experiment:
		
		\begin{itemize}
			\item \textbf{Type of Devices:} The type of devices in the smart home does not change throughout the experiment.
			\item \textbf{Number of Devices:} The number of devices in the smart home does not change throughout the experiment.
			\item \textbf{Number of Users:} The number of smart home occupants does not change throughout the experiment.
			\item \textbf{Location of Sniffers:} The location of the sniffers relative to the smart home devices does not change throughout the experiment.
			\item \textbf{Location of Devices:} The location of the devices relative to the sniffers does not change throughout the experiment.
			\item \textbf{Computing Parameters:} The operating systems, resources (memory, CPU, and disk space), script languages, and hardware are held constant.
		\end{itemize}
		
		\section{Experimental Design}
		
		The purpose of this experiment is to meet the four objectives listed above. The experiment scenario is defined by a user performing actions from a script to interact with the smart home environment. These events occur while the user is both in and away from the smart home environment. Data logging occurs to provide truth data used to evaluate the \ac{CITIoT} tool's operation.
		
			\subsection{\acf{SHAA}}
			
			Figure~\ref{fig:ShaaExperimentDiagram} depicts how the devices, \ac{CITIoT} tool, and \ac{MIoTL} tool are laid out within \ac{SHAA}. To provide consistency between trials, all devices, excluding the iPhone, are not moved throughout experimentation. Proximity between the devices and \ac{CITIoT} tool provides greater chances of packet capture for testing.
			
			\figShaaExperimentDiagram
			
			\subsection{\ac{CITIoT}}
			Figure~\ref{fig:ShaaExperimentDiagram} shows where the \ac{CITIoT} tool is placed within \ac{SHAA}. Each sniffer operates in the 2.4 GHz band and must be horizontally isolated to avoid interference. The distance between antennae, $d$, to provide horizontal isolation can be expressed by the equation
			\begin{equation}
			d\geq 2 \frac{D^2}{\lambda}
			\end{equation}
			where D is the length of the antenna in meters and $\lambda$ is the wavelength of the device frequency band in Hz \cite{antenna}.
			
			The Ubertooth One antennae are 3.5 inches long and operate with an average wavelength of 2441 MHz, while the Alfa Card antenna is 6.5 inches long and operates with an average wavelength of 2412 MHz. Plugging these values into the equation provides a separation value of about 5 inches for the Ubertooth One antennae and 17 inches for the Alfa Card antenna. Figure~\ref{fig:SnifferExperimentSetup} shows how the individual sniffers are setup to avoid horizontal interference. The Ubertooth One sniffers are separated by 11 inches, while the Alfa Card is 20 to 23 inches from each of the Ubertooth One sniffers.
			
			\figSnifferExperimentSetup
			
			\subsection{Treatments} \label{treatments}
			Table~\ref{tbl:Events} lists the thirty-one events used during experimentation which occur randomly during trials. Events happen multiple times during a trial at random intervals. Each event allows the \ac{CITIoT} tool to be evaluated against different devices and actions.
			
			\tableEvents
			
			The events are administered with \ac{SHAA} operating in two states: with and without the \ac{MIoTL} tool activated. Table~\ref{tbl:Treatments} describes how the treatments are administered among trials.
			
			\tableTreatments
			
			\subsection{Logging}
			A user log is used to record the time, device name, and action of each event carried out in \ac{SHAA} during experimentation. Events recorded include turning on and off loggers, starting and stopping sniffers, arriving or leaving the smart home, computer errors, and activating devices. Additionally, the Raspberry Pi records each Wi-Fi event processed by the Homebridge server. These two logs are considered truth data and used to calculate the \ac{DCSR}, \ac{EISR}, \ac{EIFP}, and \ac{EIFN} response variables.
			
			\subsection{Testing Process}
			Trials are carried out over six ten-hour days. The Homebridge logger and \ac{CITIoT} sniffers are activated at the beginning of each trial. At least one minute is allowed before events occur to permit the logger and sniffers time to normalize. Then, each event from Table~\ref{tbl:Events} is carried out in a random order in the morning and again in the evening. Also, devices are randomly activated throughout the day while the user is away. The time of each treatment is recorded in the user log. At the end of the day, \ac{CITIoT} sniffers and the Homebridge logger are deactivated and the processing unit of the \ac{CITIoT} tool is started. When complete, the classifier, \ac{MAC} tracker, and network mapper units are activated. Timing is built into the \ac{CITIoT} tool using Python's time module for each unit to provide the wall-clock time for the response variable, \ac{PT}. Results are stored for statistical analysis and evaluation.
			
			The testing process is repeated during one trial with the \ac{MIoTL} tool operating. As \ac{MIoTL} only creates Wi-Fi traffic to impede with the \ac{CITIoT} tool's operation, a subset of treatments only including Wi-Fi events are used. The \ac{MIoTL} tool is activated at least five minutes prior to the Homebridge logger and the \ac{CITIoT} Wi-Fi sniffer to allow for normalization. Wi-Fi devices are activate and the user log is maintained as in the first trials. At the end of the day, the \ac{CITIoT} Wi-Fi sniffer, the Homebridge logger, and \ac{MIoTL} are deactivated. The \ac{CITIoT} tool operates similarly to previous trials after that.
			
		\section{Statistical Analysis}
		Data is collected through three main components: (i) results from the \ac{CITIoT} tool, (ii) the logger on the Raspberry Pi recording events processed by the Homebridge server, and (iii) the user logs. This data is imported into the statistical analysis tool R, a GNU project language for statistical computing, and the \ac{CITIoT} results are compared to the two truth data sources. The \ac{DCSR}, \ac{EISR}, \ac{ULSR}, \ac{PT}, and {HDU} data are tested for mean validity using a one-sample t-test, and computing the standard deviation, mean, and 95\% confidence interval.
		
		The comparison between the \ac{CITIoT} tool's operation with and without the \ac{MIoTL} tool running is accomplished using a paired t-test. If proven significant, the mean of the five trials without the \ac{MIoTL} tool running is compared to the one trial with the tool operational.
		
		\section{Methodology Summary}
		This chapter describes the experimentation methodology used to measure the efficiency (\ac{PT} and \ac{HDU}) and accuracy (\ac{DCSR}, \ac{EISR}, and \ac{ULSR}) of the \ac{CITIoT} tool. The treatments allow for various devices and actions to determine the operational capabilities of the tool. The effectiveness of the \ac{MIoTL} tool in mitigating some of the \ac{CITIoT} tool's features is measured through the accuracy of the \ac{CITIoT} tool while the \ac{MIoTL} tool is operating.
		
\backmatter
	\singlespace
	\bibliographystyle{plain}
	\bibliography{../Back/myReferences} 
	\clearpage

\end{document}

